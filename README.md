# LModel-Optimization ğŸ§ âš¡

**Optimizing AI Models for Real-World Performance**

This project is focused on making AI models faster, smaller, and more efficient â€” especially for deployment in resource-constrained environments. The goal is to reduce inference time and memory footprint **without sacrificing accuracy**, making models practical for real-world use cases such as production APIs, mobile, and edge devices.

> ğŸ”— **Read the full article** for a deep dive into the theory, motivation, and implementation steps:  
[Optimizing AI Models for Real-World Performance: Tuning for Faster Inference](https://medium.com/@eddyejembi/optimizing-ai-models-for-real-world-performance-tuning-for-faster-inference-8d01b35b04d0)

---

## ğŸš€ What This Project Covers

This repo contains practical examples and code implementations for:

- **Quantization** â€” reducing model size and speeding up inference by converting weights from float32 to int8/float16.
- **Pruning** â€” removing unnecessary weights and neurons while maintaining performance.
- **Knowledge Distillation** â€” training smaller â€œstudentâ€ models using outputs from a larger â€œteacherâ€ model.
- **ONNX Export & Runtime Optimization** â€” converting models to ONNX format and using inference-optimized runtimes.
- **Benchmarking Tools** â€” to compare inference speed and model sizes before and after optimization.

---

## ğŸ§© Structure

```bash
LModel-Optimization/
â”œâ”€â”€ base_bot.py
â”œâ”€â”€ chatbot.py
â”œâ”€â”€ load_model.py
â”œâ”€â”€ optim_bot.py
â”œâ”€â”€ optimum_optimize.py
â””â”€â”€ requirements
```

## ğŸ§ª Experimenting
### ğŸ“¦ Installation
```
git clone https://github.com/EddyEjembi/LModel-Optimization.git
cd LModel-Optimization
pip install -r requirements.txt
```
### Running the Code
- **`load_model.py`** â€” Downloads the *`meta-llama/Llama-3.2-1B-Instruct`* model to serve as the base model.
- **`optimum_optimize.py`** â€” Optimizes and quantizes the model using HuggingFace Optimum.
- **`base_bot.py`** â€” Runs inference on the base model.
- **`optim_bot.py`** â€” Runs inference on the optimized model of your choice:

    ```
    # Load Models
    onnx_model_path = "onnx_model" # Directory for ONNX Model
    optimized_onnx_model_path = "optimized_onnx_model"  # Directory for Optimized ONNX model
    quantized_model_path = "quantized_onnx_model" # Directory to Quantized Model

    model = ORTModelForCausalLM.from_pretrained(optimized_onnx_model_path)
    ```

## ğŸ“š Learn More
For a full walkthrough and real-world context, check out the article:

ğŸ‘‰ [Optimizing AI Models for Real-World Performance](https://medium.com/@eddyejembi/optimizing-ai-models-for-real-world-performance-tuning-for-faster-inference-8d01b35b04d0)

## ğŸ™Œ Contributing
Pull requests are welcome! If you have better optimization techniques, benchmarks, or use cases â€” feel free to fork and contribute and connect with me on any of the platform:
- ğŸ¦ [X (Twitter)](https://x.com/eddyejembi)
- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/eddyejembi/)
- ğŸ“¬ [Mail](mailto:eddyejembi2018@gmail.com)
